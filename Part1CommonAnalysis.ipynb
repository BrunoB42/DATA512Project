{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21cea870-3085-4e15-90ff-b81904b59b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import copy\n",
    "import fiona\n",
    "import requests\n",
    "import re\n",
    "import folium.folium\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Polygon, Point\n",
    "from geopandas.explore import _categorical_legend\n",
    "from folium.plugins import Geocoder\n",
    "from matplotlib.colors import rgb2hex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa08ef76-c109-4c9c-b36f-37cb1fa887a1",
   "metadata": {},
   "source": [
    "In the first part of this analysis, an estimated metric of smoke impact on the city of Shreveport will be devised, and a time series model will be developed to forecast this metric into the next 25 years. This smoke estimate will be compared alongside an estimate of daily and yearly AQI in Shreveport in order to both validate the effectiveness of estimated smoke impact and to highlight the ways in which AQI fails as a measurement of wildfire smoke pollution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ec351-b4bd-4404-9841-92abdcfe9958",
   "metadata": {},
   "source": [
    "# Part 1: Air Quality Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e9e0e-0f52-4b03-967b-b7953ccb7a67",
   "metadata": {},
   "source": [
    "In order to understand the effectiveness of our analysis' future estimates of smoke impact, it is essential that we first prepare an estimate of the AQI for our city of interest. This AQI shall serve as a point of reference for future analysis. In order to obtain this, we will use the EPA's AQS API, a service that provides historical air quality data for areas across the country. This data is obtained from the various monitoring stations across the U.S. and is associated with given regions by the EPA itself. Through this API, we can identify the monitoring stations closest to Shreveport and retrieve the air quality data from each of them, in order to form a more accurate estimate of the AQI of the city as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b6cff8-0fb1-4ea8-ac29-0a0cba4bd673",
   "metadata": {},
   "source": [
    "### Obtaining Monitoring Station Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca1756-5a81-4fd4-81e1-6a1afcf0dcfc",
   "metadata": {},
   "source": [
    "The following blocks of code were developed by Dr.McDonald for the purposes of accessing the AQS API and and is used here under its CC-BY license in order to more easily obtain the AQI data for Shreveport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b58070-1cbc-4aa8-adaf-abbdcd06716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "#\n",
    "#    This is the root of all AQS API URLs\n",
    "#\n",
    "API_REQUEST_URL = 'https://aqs.epa.gov/data/api'\n",
    "\n",
    "#\n",
    "#    These are some of the 'actions' we can ask the API to take or requests that we can make of the API\n",
    "#\n",
    "#    Sign-up request - generally only performed once - unless you lose your key\n",
    "API_ACTION_SIGNUP = '/signup?email={email}'\n",
    "#\n",
    "#    List actions provide information on API parameter values that are required by some other actions/requests\n",
    "API_ACTION_LIST_CLASSES = '/list/classes?email={email}&key={key}'\n",
    "API_ACTION_LIST_PARAMS = '/list/parametersByClass?email={email}&key={key}&pc={pclass}'\n",
    "API_ACTION_LIST_SITES = '/list/sitesByCounty?email={email}&key={key}&state={state}&county={county}'\n",
    "#\n",
    "#    Monitor actions are requests for monitoring stations that meet specific criteria\n",
    "API_ACTION_MONITORS_COUNTY = '/monitors/byCounty?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&state={state}&county={county}'\n",
    "API_ACTION_MONITORS_BOX = '/monitors/byBox?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'\n",
    "#\n",
    "#    Summary actions are requests for summary data. These are for daily summaries\n",
    "API_ACTION_DAILY_SUMMARY_COUNTY = '/dailyData/byCounty?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&state={state}&county={county}'\n",
    "API_ACTION_DAILY_SUMMARY_BOX = '/dailyData/byBox?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'\n",
    "#\n",
    "#    It is always nice to be respectful of a free data resource.\n",
    "#    We're going to observe a 100 requests per minute limit - which is fairly nice\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "#\n",
    "#\n",
    "#    This is a template that covers most of the parameters for the actions we might take, from the set of actions\n",
    "#    above. In the examples below, most of the time parameters can either be supplied as individual values to a\n",
    "#    function - or they can be set in a copy of the template and passed in with the template.\n",
    "# \n",
    "AQS_REQUEST_TEMPLATE = {\n",
    "    \"email\":      \"\",     \n",
    "    \"key\":        \"\",      \n",
    "    \"state\":      \"\",     # the two digit state FIPS # as a string\n",
    "    \"county\":     \"\",     # the three digit county FIPS # as a string\n",
    "    \"begin_date\": \"\",     # the start of a time window in YYYYMMDD format\n",
    "    \"end_date\":   \"\",     # the end of a time window in YYYYMMDD format, begin_date and end_date must be in the same year\n",
    "    \"minlat\":    0.0,\n",
    "    \"maxlat\":    0.0,\n",
    "    \"minlon\":    0.0,\n",
    "    \"maxlon\":    0.0,\n",
    "    \"param\":     \"\",     # a list of comma separated 5 digit codes, max 5 codes requested\n",
    "    \"pclass\":    \"\"      # parameter class is only used by the List calls\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e0c0f9b-f361-4d0e-b1cd-89996649e383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#    This implements the list request. There are several versions of the list request that only require email and key.\n",
    "#    This code sets the default action/requests to list the groups or parameter class descriptors. Having those descriptors \n",
    "#    allows one to request the individual (proprietary) 5 digit codes for individual air quality measures by using the\n",
    "#    param request. Some code in later cells will illustrate those requests.\n",
    "#\n",
    "def request_list_info(email_address = None, key = None,\n",
    "                      endpoint_url = API_REQUEST_URL, \n",
    "                      endpoint_action = API_ACTION_LIST_CLASSES, \n",
    "                      request_template = AQS_REQUEST_TEMPLATE,\n",
    "                      headers = None):\n",
    "    \n",
    "    #  Make sure we have email and key - at least\n",
    "    #  This prioritizes the info from the call parameters - not what's already in the template\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address\n",
    "    if key:\n",
    "        request_template['key'] = key\n",
    "    \n",
    "    # For the basic request we need an email address and a key\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_list_info()'\")\n",
    "    if not request_template['key']: \n",
    "        raise Exception(\"Must supply a key to call 'request_list_info()'\")\n",
    "\n",
    "    # compose the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "        \n",
    "    # make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e03ad5e-2618-4d68-a135-09a22d5a502b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"code\": \"42101\",\n",
      "        \"value_represented\": \"Carbon monoxide\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"42401\",\n",
      "        \"value_represented\": \"Sulfur dioxide\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"42602\",\n",
      "        \"value_represented\": \"Nitrogen dioxide (NO2)\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"44201\",\n",
      "        \"value_represented\": \"Ozone\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"81102\",\n",
      "        \"value_represented\": \"PM10 Total 0-10um STP\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"88101\",\n",
      "        \"value_represented\": \"PM2.5 - Local Conditions\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"88502\",\n",
      "        \"value_represented\": \"Acceptable PM2.5 AQI & Speciation Mass\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#   Once we have a list of the classes or groups of possible sensors, we can find the sensor IDs that make up that class (group)\n",
    "#   The one that looks to be associated with the Air Quality Index is \"AQI POLLUTANTS\"\n",
    "#   We'll use that to make another list request.\n",
    "#\n",
    "AQI_PARAM_CLASS = \"AQI POLLUTANTS\"\n",
    "\n",
    "#\n",
    "#   Structure a request to get the sensor IDs associated with the AQI\n",
    "#\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['pclass'] = AQI_PARAM_CLASS  # here we specify that we want this 'pclass' or parameter classs\n",
    "\n",
    "response = request_list_info(request_template=request_data, endpoint_action=API_ACTION_LIST_PARAMS)\n",
    "\n",
    "if response[\"Header\"][0]['status'] == \"Success\":\n",
    "    print(json.dumps(response['Data'],indent=4))\n",
    "else:\n",
    "    print(json.dumps(response,indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f9cdd36-2014-4eda-b507-a2516b61ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Given the set of sensor codes, now we can create a parameter list or 'param' value as defined by the AQS API spec.\n",
    "#   It turns out that we want all of these measures for AQI, but we need to have two different param constants to get\n",
    "#   all seven of the code types. We can only have a max of 5 sensors/values request per param.\n",
    "#\n",
    "#   Gaseous AQI pollutants CO, SO2, NO2, and O2\n",
    "AQI_PARAMS_GASEOUS = \"42101,42401,42602,44201\"\n",
    "#\n",
    "#   Particulate AQI pollutants PM10, PM2.5, and Acceptable PM2.5\n",
    "AQI_PARAMS_PARTICULATES = \"81102,88101,88502\"\n",
    "#   \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843e0f0-23d6-4d8b-8227-aa22073f2f43",
   "metadata": {},
   "source": [
    "In order to use the Shreveport AQI as a point of reference for our smoke impact metric, we must first confirm which monitoring stations are present near Shreveport and ensure that sufficient data is likely to be available. Though Shreveport is likely to have ample data (being the third-largest city in the state), the unconventional way in which Louisiana splits itself into parishes ought to give enough pause to make this worth inspecting directly\n",
    "\n",
    "The following code block uses the FIPS code of the Caddo parish that makes up the majority of Shreveport to make an API call requesting a list of monitoring sites within this parish. For other states, this API relies on county information (where it is available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a511a00-ffff-4500-9419-ed06da6669d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"code\": \"0001\",\n",
      "        \"value_represented\": \"Dixie\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0002\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0003\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0004\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0005\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0006\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0007\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0008\",\n",
      "        \"value_represented\": \"Shreveport / Calumet\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"1001\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"1002\",\n",
      "        \"value_represented\": \"CLAIBORNE SITE\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"5501\",\n",
      "        \"value_represented\": \"Eden Gardens Fundamental Elementary School\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['state'] = '22'   # the first two digits (characters) of FIPS is the state code\n",
    "request_data['county'] = '017'  # the last three digits (characters) of FIPS is the county code\n",
    "\n",
    "response = request_list_info(request_template=request_data, endpoint_action=API_ACTION_LIST_SITES)\n",
    "\n",
    "if response[\"Header\"][0]['status'] == \"Success\":\n",
    "    print(json.dumps(response['Data'],indent=4))\n",
    "else:\n",
    "    print(json.dumps(response,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613e59a6-29cf-4f56-8bea-ce5a238f8965",
   "metadata": {},
   "source": [
    "From this, it can be seen that Shreveport has its own monitoring station and is surrounded by 2 other active monitoring stations. Due to Louisiana's unconventional subdivision methods, the majority of the stations returned by the API are unnamed and will provide no AQI data when queried. They are not of interest to our analysis. However, the three that remain all collectively encompass the the Caddo parish and thus provide ample data for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464ae7f-ed15-411b-ad85-c4a9a09fa218",
   "metadata": {},
   "source": [
    "### Obtaining Daily Summary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb76e75-9c55-4c1f-85a9-4c2a42b915fb",
   "metadata": {},
   "source": [
    "With the dataset's viability confirmed, we can now use the AQS API to request the daily summary data for our region. For the purposes of our analysis, we are strictly concerned with data from the last 60 years. That is, data between 1964 and 2024. In addition, we will be narrowing the scope of our analysis to the fire season when smoke has the biggest effect on people in the area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00a7ffd5-9b73-4adf-a531-a72db4066f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#    This implements the daily summary request. Daily summary provides a daily summary value for each sensor being requested\n",
    "#    from the start date to the end date. \n",
    "#\n",
    "#    Like the two other functions, this can be called with a mixture of a defined parameter dictionary, or with function\n",
    "#    parameters. If function parameters are provided, those take precedence over any parameters from the request template.\n",
    "#\n",
    "def request_daily_summary(email_address = None, key = None, param=None,\n",
    "                          begin_date = None, end_date = None, fips = None,\n",
    "                          endpoint_url = API_REQUEST_URL, \n",
    "                          endpoint_action = API_ACTION_DAILY_SUMMARY_COUNTY, \n",
    "                          request_template = AQS_REQUEST_TEMPLATE,\n",
    "                          headers = None):\n",
    "    \n",
    "    #  This prioritizes the info from the call parameters - not what's already in the template\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address\n",
    "    if key:\n",
    "        request_template['key'] = key\n",
    "    if param:\n",
    "        request_template['param'] = param\n",
    "    if begin_date:\n",
    "        request_template['begin_date'] = begin_date\n",
    "    if end_date:\n",
    "        request_template['end_date'] = end_date\n",
    "    if fips and len(fips)==5:\n",
    "        request_template['state'] = fips[:2]\n",
    "        request_template['county'] = fips[2:]            \n",
    "\n",
    "    # Make sure there are values that allow us to make a call - these are always required\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_daily_summary()'\")\n",
    "    if not request_template['key']: \n",
    "        raise Exception(\"Must supply a key to call 'request_daily_summary()'\")\n",
    "    if not request_template['param']: \n",
    "        raise Exception(\"Must supply param values to call 'request_daily_summary()'\")\n",
    "    if not request_template['begin_date']: \n",
    "        raise Exception(\"Must supply a begin_date to call 'request_daily_summary()'\")\n",
    "    if not request_template['end_date']: \n",
    "        raise Exception(\"Must supply an end_date to call 'request_daily_summary()'\")\n",
    "    # Note we're not validating FIPS fields because not all of the daily summary actions require the FIPS numbers\n",
    "        \n",
    "    # compose the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "        \n",
    "    # make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "\n",
    "    # Unpacking response data\n",
    "    if json_response[\"Header\"][0]['status'] == \"Success\":\n",
    "        return json_response['Data']\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ac49772-6a1c-4733-9604-7f42d1a189dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Requesting AQI data: 100%|██████████| 61/61 [04:05<00:00,  4.02s/it]\n"
     ]
    }
   ],
   "source": [
    "# Constructing AQS API daily summary request template\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['state'] = '22'\n",
    "request_data['county'] = '017'\n",
    "\n",
    "# Defining years of interest and fire season date ranges\n",
    "year_range = (1964, 2024)\n",
    "fire_season_date_range = (\"0501\", \"1031\")\n",
    "\n",
    "# Requesting fire season AQI data for every year of interest\n",
    "gas_aqi_list = []\n",
    "particulate_aqi_list = []\n",
    "for year in tqdm(range(year_range[0], year_range[1]+1), desc=\"Requesting AQI data\"):\n",
    "    start_date = f\"{year}{fire_season_date_range[0]}\"\n",
    "    end_date = f\"{year}{fire_season_date_range[1]}\"\n",
    "    # Obtaining gaseous data\n",
    "    request_data['param'] = AQI_PARAMS_GASEOUS\n",
    "    gas_aqi_data = request_daily_summary(request_template=request_data,\n",
    "                                        begin_date=start_date,\n",
    "                                        end_date=end_date)\n",
    "    if gas_aqi_data:\n",
    "        gas_aqi_list.extend(gas_aqi_data)\n",
    "    # Obtaining particulate data\n",
    "    request_data['param'] = AQI_PARAMS_PARTICULATES\n",
    "    particulate_aqi_data = request_daily_summary(request_template=request_data,\n",
    "                                            begin_date=start_date,\n",
    "                                            end_date=end_date)\n",
    "    if particulate_aqi_data:\n",
    "        particulate_aqi_list.extend(particulate_aqi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df658fb9-f8d2-4246-9150-c6abfb22dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling gas and particulate AQI datasets into DataFrames\n",
    "gas_aqi_df = pd.DataFrame.from_records(gas_aqi_list)\n",
    "particulate_aqi_df = pd.DataFrame.from_records(particulate_aqi_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07935b1-cae1-4ab0-88d8-f66335ebfe40",
   "metadata": {},
   "source": [
    "Skimming through the datset, we can observe several things of note. Firstly, only the three monitoring stations that had indentifying names have returned any data at all, as we expected. These three stations are all located very close to each other, within the bounds of Shreveport itself. As such, the results from each station will be treated equally as each measures a different corner of the town.\n",
    "\n",
    "Somewhat concerningly, all three stations occasionally report AQI values multiple times within the same day. Since our analysis occurs at the level of days and not hours, it may pose an issue for our future model if these values do not agree with each other and end up invalidating the dataset. Closer inspection reveals that some values _do_ in fact disagree with each other to some degree. However, only ~120 entries in this datset of ~24000 entries occur within the same day and station. As such, this issue can safely be ignored as insignificant as it comprises less than 0.5% of the dataset.\n",
    "\n",
    "Of greater issue is the fact that the datasets are simply far too dense. Several columns are present that are of no use to our analysis. To improve the clarity of our dataset, it is important to restrict our analysis to only our columns of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9e799a0-5793-4fd7-a364-c95d75d17bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining columns of interest\n",
    "cols_of_interest = [\n",
    "    \"site_number\", # Identifier for the site that the data was obtained from\n",
    "    \"parameter_code\", # 5-digit code identifying the parameter being measured\n",
    "    \"poc\", # Parameter Occurence Code. Identifies individual pollutant monitoring devices for locations with more than one\n",
    "    \"latitude\", # Latitude of the monitoring station\n",
    "    \"longitude\", # Longitude of the monitoring station\n",
    "    \"parameter\", # The pollutant value being measured\n",
    "    \"sample_duration\", # Duration of measurement\n",
    "    \"date_local\", # Local measurement date\n",
    "    \"units_of_measure\", # units that the pollutant measure is given in\n",
    "    \"aqi\", # Air Quality Index\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efbdb409-2ef4-4977-b157-962d9c3062da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing irrelevant columns and combining datasets\n",
    "aqi_df = pd.concat([gas_aqi_df[cols_of_interest],\n",
    "                    particulate_aqi_df[cols_of_interest]],\n",
    "                    ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf5bd6e-7e92-4ae8-8db8-1b4056d4cc6f",
   "metadata": {},
   "source": [
    "### Estimating AQI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6717c4-a686-4d5d-9755-910c601816a8",
   "metadata": {},
   "source": [
    "With the dataset trimmed down to its essentials, we can make our rudimentary AQI estimate. Ultimately, this analysis is limited by the fact that it is impossible to know from the dataset, which particulates came from fires and which came from other sources. Indeed, prior research into the expected composition of wildfire smoke would be needed in order to narrow down the pollutants of concern even _if_ the data could somehow be assocated to it. For lack of a better point of referennce however, we can proceed by making a set of assumptions.\n",
    "\n",
    "When a wildfire does occur, we can assume that its smoke output will compose the vast majority of the of the pollutants in the air. We can also assume that these particulates will undergo strictly passive diffusion into the surrounding area. And lastly, we can assume that the most severe pollutant (using AQI as a measure of severity) makes up the majority of the regional AQI for any given day.\n",
    "\n",
    "With this, we can use the daily AQI as an point of reference for our analysis and we can estimate the daily AQI as simply being the AQI of the most severe pollutant. The highest AQI measured for any given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1868155a-a117-403b-9259-1159ab082ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating daily AQI as the highest AQI among pollutants in each day\n",
    "aqi_est_df_daily = aqi_df[['date_local', 'aqi']].groupby(by='date_local').max().reset_index()\n",
    "aqi_est_df_daily['date_local'] = pd.to_datetime(aqi_est_df_daily['date_local'])\n",
    "aqi_est_df_daily['year'] = aqi_est_df_daily['date_local'].dt.year\n",
    "aqi_est_df_daily = aqi_est_df_daily.sort_values(by='date_local').reset_index(drop=True)\n",
    "# Calculating yearly AQI as an average of daily AQI\n",
    "aqi_est_df_yearly = aqi_est_df_daily.groupby(by='year').mean().reset_index().drop(columns='date_local', inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc815b65-3091-41a4-9379-cb9c5a627f1b",
   "metadata": {},
   "source": [
    "# Part 2: Wildfire Smoke Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd9f3a-9aa4-46fa-850a-bba6d890c924",
   "metadata": {},
   "source": [
    "With our reference AQI estimates now obtained, we can generate our own smoke estimates from the local wildfire data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edd1fd-15b3-4a2e-89c0-2815444ea837",
   "metadata": {},
   "source": [
    "### Obtaining Wildfire Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cad20a-8530-4d67-9dbb-3025112ec28f",
   "metadata": {},
   "source": [
    "The wildfire data used in this analysis is sourced from the US Geological Survey's Wildland fire dataset. Specifically, this analysis will be performed with the `USGS_Wildland_Fire_Combined_Dataset` contained within the Wildland geodatabase file.\n",
    "\n",
    "The full geodatabase file contains a dataset of unprocessed merged data from the Geological Survey's various sources (with redundant entries), a dataset of overlapping entries between its sources, and a fully cleaned \"Combined\" dataset. As this analysis has no need of the raw data, the combined dataset will be used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e71e05e-f2b5-47bf-870f-696ff3234541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in Wildland dataset\n",
    "wildfire_gdf: gpd.GeoDataFrame = gpd.read_file(\"wildfire_data/Fire_Feature_Data.gdb\", layer=\"USGS_Wildland_Fire_Combined_Dataset\")\n",
    "# Generating geospatial point for Shreveport\n",
    "# (Please excuse the pun. I simply had to)\n",
    "shrevepoint = Point(-93.763504, 32.523659)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78dca1f-5994-462c-9af6-b74f3014180b",
   "metadata": {},
   "source": [
    "This dataset contains data for the entire continental U.S. and is thus wildly excessive for our purposes. When making estimates of smoke impacts on Shreveport, we need only consider the relevant data. That being, data from the last 60 years, during the fire season, and within 650 miles of Shreveport itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e65ad79e-9db4-407a-aa4f-af356a535d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict dataset to fires in the last 60 years\n",
    "wildfire_gdf: gpd.GeoDataFrame = wildfire_gdf[wildfire_gdf['Fire_Year'] >= year_range[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04634f-a698-489e-823f-a5f0266d3c0f",
   "metadata": {},
   "source": [
    "Throughout this analysis, two main reference systems will be used. When attempting to plot a set of coordinates onto a flat map, the default **EPSG 4326** coordinate system will be used as it is the coordinate system expected by geopandas' plotting functions (and the only coordinate system to which the Wildland dataset's ESRI format can be immediately converted to by geopandas). However, when calculating distances to estimate the spread of wildfire smoke, the **EPSG 5070** system will be used instead as it properly accounds for the curvature of the earth when making comparisons between points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "54cca1bd-b2b4-4553-b29b-c091d42b457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dataset from ESRI to EPSG format\n",
    "wildfire_gdf: gpd.GeoDataFrame = wildfire_gdf.to_crs(epsg=4326)\n",
    "shrevepoint_epsg = gpd.GeoSeries([shrevepoint], crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "317e4663-3889-4a44-a1af-51e028da96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating distance and converting to kilometers\n",
    "wildfire_gdf[\"dist_epsg_5070\"] = wildfire_gdf.to_crs(epsg=5070).distance(shrevepoint_epsg.to_crs(epsg=5070)[0]) * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "018fbbf6-6c40-4867-8d6e-7dda21c231d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out fires over 1046km (650mi) away\n",
    "wildfire_gdf = wildfire_gdf[wildfire_gdf[\"dist_epsg_5070\"] <= 1046]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441d444-79db-4e4f-8932-47440ade1ff2",
   "metadata": {},
   "source": [
    "With the distances and years accounted for, the wildfire data must now be filtered for the fire season. As each entry in the dataset contains multiple date keys that could each be used to roughly estimate whether or not it occured during the fire season. In order to filter by this, we must take the latest fire end date available and the earliest fire start date available and ensure that they are less than October 31st and greater than May 1st respectively.\n",
    "\n",
    "The following three functions perform these operations in a time-efficient manner. **See README for attribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57d1f0c5-a4a7-4627-9634-0262e1b1aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRE_FIRST_DATE_COL = 'Fire_First_Date'\n",
    "FIRE_LAST_DATE_COL = 'Fire_Last_Date'\n",
    "\n",
    "DEFAULT_FIRST_DD_MM = '05-01'\n",
    "DEFAULT_LAST_DD_MM = '10-31'\n",
    "\n",
    "FIRST_DATE_POSSIBLE_KEYS = [\n",
    "    'Listed Ignition Date(s)',\n",
    "    'Listed Wildfire Discovery Date(s)',\n",
    "    'Listed Prescribed Fire Start Date(s)',\n",
    "    'Prescribed Fire Start Date'\n",
    "    'Listed Other Fire Date(s)'\n",
    "]\n",
    "first_date_keys_pattern = '|'.join(re.escape(key) for key in FIRST_DATE_POSSIBLE_KEYS)\n",
    "first_date_pattern = rf'({first_date_keys_pattern}):\\s*(\\d{{4}}-\\d{{2}}-\\d{{2}})\\b'\n",
    "\n",
    "def get_first_dates(fire_feature_row, default_start_dd_mm=DEFAULT_FIRST_DD_MM, default_year=START_YEAR):\n",
    "    \"\"\"\n",
    "    Get the start date of the fire from Listed_Fire_Dates.\n",
    "    Currently, checks if any one of the date keys is in the list of FIRST_DATE_POSSIBLE_KEYS (in order).\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    fire_feature_row (pd.Series): A row from the fire feature GeoDataFrame.\n",
    "\n",
    "    default_start_dd_mm (str): The default start date (MM-DD) to use if no valid date is found.\n",
    "\n",
    "    default_year (int): The default year to use if no valid date is found.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    str: The first date of the fire feature.\n",
    "    \"\"\"\n",
    "    # Check if the fire feature row has a valid date\n",
    "    match = re.search(first_date_pattern, fire_feature_row['Listed_Fire_Dates'])\n",
    "\n",
    "    date = None\n",
    "    # If a match is found, return the date string\n",
    "    if match:\n",
    "        date = match.group(2)  # Group 2 captures the date\n",
    "    else:\n",
    "        date = f'{fire_feature_row['Fire_Year'] or default_year}-{default_start_dd_mm}'\n",
    "    return date\n",
    "\n",
    "LAST_DATE_POSSIBLE_KEYS = [\n",
    "    'Listed Wildfire Controlled Date(s)',\n",
    "    'Listed Wildfire Containment Date(s)'\n",
    "    'Listed Wildfire Out Date(s)',\n",
    "    'Prescribed Fire End Date',\n",
    "]\n",
    "last_date_keys_pattern = '|'.join(re.escape(key) for key in LAST_DATE_POSSIBLE_KEYS)\n",
    "last_date_pattern = rf'({last_date_keys_pattern}):\\s*(\\d{{4}}-\\d{{2}}-\\d{{2}})\\b'\n",
    "\n",
    "def get_last_dates(fire_feature_row, default_end_dd_mm=DEFAULT_LAST_DD_MM, default_year=START_YEAR):\n",
    "    \"\"\"\n",
    "    Get the end date of the fire from Listed_Fire_Dates.\n",
    "    Currently, checks if any one of the date keys is in the list of LAST_DATE_POSSIBLE_KEYS (in order).\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    fire_feature_row (pd.Series): A row from the fire feature GeoDataFrame.\n",
    "\n",
    "    default_end_dd_mm (str): The default end date (MM-DD) to use if no valid date is found.\n",
    "\n",
    "    default_year (int): The default year to use if no valid date is found.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    str: The last date of the fire feature.\n",
    "    \"\"\"\n",
    "    # Check if the fire feature row has a valid date\n",
    "    match = re.search(last_date_pattern, fire_feature_row['Listed_Fire_Dates'])\n",
    "\n",
    "    date = None\n",
    "    # If a match is found, return the date string\n",
    "    if match:\n",
    "        date = match.group(2)  # Group 2 captures the date\n",
    "    else:\n",
    "        date = f'{fire_feature_row['Fire_Year'] or default_year}-{default_end_dd_mm}'\n",
    "    return date\n",
    "\n",
    "def overlap_with_fire_season(row):\n",
    "    \"\"\"\n",
    "    Check if the fire overlaps with the fire season (May 1 - October 31).\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    row (pd.Series): A row from the fire feature GeoDataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    bool: True if the fire overlaps with the fire season, False otherwise.\n",
    "    \"\"\"\n",
    "    fire_first_date = row[FIRE_FIRST_DATE_COL]\n",
    "    fire_last_date = row[FIRE_LAST_DATE_COL]\n",
    "    \n",
    "    # Define the fire season start and end dates\n",
    "    fire_season_start = pd.to_datetime(f'{row['Fire_Year']}-05-01')\n",
    "    fire_season_end = pd.to_datetime(f'{row['Fire_Year']}-10-31')\n",
    "    \n",
    "    # Check for overlap\n",
    "    return (fire_first_date <= fire_season_end) and (fire_last_date >= fire_season_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127fd60-3245-45c9-9305-d6ccf93037cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing entries with no date data to pull from\n",
    "wildfire_gdf = wildfire_gdf.dropna(subset='Listed_Fire_Dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1346f420-8a9b-465a-84ca-32e2d90ae863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the above sourced functions in order to obtain the first and last listed dates for each entry\n",
    "wildfire_gdf[FIRE_FIRST_DATE_COL] = pd.to_datetime(wildfire_gdf.apply(get_first_dates, axis=1))\n",
    "wildfire_gdf[FIRE_LAST_DATE_COL] = pd.to_datetime(wildfire_gdf.apply(get_last_dates, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c811bcb5-c6d5-4b95-b2e1-1c395dc6cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for fire season entries\n",
    "wildfire_gdf = wildfire_gdf[wildfire_gdf.apply(overlap_with_fire_season, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164316c-56a5-47b7-90de-242a87e2db1d",
   "metadata": {},
   "source": [
    "### Part 3: Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6808fd53-da45-4dfa-bbd4-c0d1d68d3428",
   "metadata": {},
   "source": [
    "In order to create an estimate of smoke impacts on Shreveport, we must first develop a rudimentary model for the relationship between smoke impact and nearby wildfires. The model for smoke impact must take into account factors in the dataset that are assumed to affect the severity of smoke on nearby cities. Within the wildland fire dataset these factors are the size of the fire, the distance of the fire from the town, and whether or not the fire was prescribed.\n",
    "\n",
    "The size, or area, of a fire is directly proportional to how much smoke it will output as a larger burning surface area is a larger smoke-producing surface area.\n",
    "\n",
    "By contrast, the distance of a fire from a city is inversely proportional to how much of its smoke we would expect to be able to reach it. In fact, if we assume that the smoke moves through passive diffusion, it is inversely proportional to the _square_ of its distance to the city.\n",
    "\n",
    "Finally, we can take note of the fact that the wildland fire dataset contains both wildfires and prescribed, controlled fires. While both of these kinds of fires can be expected to generate similar amounts of smoke (controlling for size), it is reasonable to assume that controlled fires are typically performed with the aim of minimizing the impact on large populated areas. As such, we would expect the smoke impact to be proportional to how uncontrolled the fire is.\n",
    "\n",
    "Very conviently for our model, the level of control a fire had is recorded as an ordinal variable where higher values indicate a more uncontrolled fire. Through this logic, we can create a rudimentary model where the smoke impact is the product of the fire area, the fire control value, and the square of the inverse of the distance to the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27e46b-1642-4c43-a702-900faf354ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "#\n",
    "# Note to self: Move modelling notebook code here before pushing\n",
    "#\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24964c1-c8d4-4ab0-91b3-530f66f13365",
   "metadata": {},
   "source": [
    "With the smoke impact estimates and the AQI estimates both obtained, we can now move on to developing a time series model that can forecast smoke impact estimates out into the next 25 years.\n",
    "\n",
    "For this purpose, a standard ARIMA model will be used and its assumption of stationary behavior will be investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7d43d-85c4-49b2-8853-bbbae7ef39ae",
   "metadata": {},
   "source": [
    "### Part 4: Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3268c-0f7b-49f0-a3ef-d3b25fb0ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "#\n",
    "# Note to self: Move analysis notebook code here before pushing\n",
    "#\n",
    "#####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
